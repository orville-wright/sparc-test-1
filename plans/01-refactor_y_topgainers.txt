Phase 1: Setup and Basic Request

check if module requests-htm is installed:


Modify Imports:
Remove from bs4 import BeautifulSoup.
Add from requests_html import HTMLSession.

Adapt Request Handling:

Decide where the HTTP request will be made. 
Let's assume we modify ext_get_data to be more self-contained for this plan. If self.ext_req must be set externally, adjust accordingly.
Instantiate HTMLSession: session = HTMLSession()

Perform the GET request: 
r = session.get(url, headers=self.yahoo_headers)
(Crucial Step - JavaScript Rendering): Yahoo Finance tables often load data via JavaScript after the initial HTML load. requests-html can handle this. After getting the response r, render the page:

try:
    # Add a timeout to render, it can sometimes hang
    r.html.render(timeout=20) 
except Exception as e:
    logging.error(f"Failed to render JavaScript: {e}")
    # Decide how to handle failure - maybe return early?
    return None # Or raise an exception

Note: The first time render() is called, it will download Chromium.



Phase 2: Replace Parsing and Element Finding

Modify ext_get_data:
Remove the line self.soup = BeautifulSoup(r.text, 'html.parser'). The parsed HTML is now available directly via r.html.
Remove self.soup as a class attribute if it's not needed elsewhere.
Replace self.tag_tbody = self.soup.find('tbody') with a CSS selector using requests-html:

# Use r.html (the parsed content after potential rendering)
self.tag_tbody = r.html.find('tbody', first=True) 
if not self.tag_tbody:
    logging.error("Could not find the <tbody> element.")
    # Handle error: maybe return or raise
    return None


Replace self.tr_rows = self.tag_tbody.find_all("tr") with:

# Find all 'tr' elements within the tbody element
self.tr_rows = self.tag_tbody.find("tr") 
# self.tr_rows is now a list of requests-html Element objects


Update logging messages to reflect the use of requests-html.
Store the requests-html Element objects (the rows) in self.tr_rows.



Phase 3: Update Data Extraction Logic (build_tg_df0)

Iterate through requests-html Elements: 
The loop for datarow in self.tr_rows: will now iterate over requests-html Element objects instead of BS4 Tag objects.

Modify extr_gen Generator:
Replace datarow.find_all("td") with datarow.find("td"). 
This returns a list of Element objects representing the cells (<td>).

Replace the check if i.canvas is not None: with a check using requests-html methods. You can check for the tag's existence:
if i.find('canvas', first=True): # Check if a canvas tag exists within the td
    yield ("canvas")


Replace next(i.stripped_strings) with the requests-html way to get clean text. element.text is usually sufficient:
else:
    # .text usually provides reasonably clean text
    yield (i.text)

Self-correction: If i.text includes unwanted whitespace or needs more aggressive stripping similar to stripped_strings, you might need i.text.strip() or potentially more processing if the text is fragmented across multiple inner tags. Test this carefully.
Verify Data Cleaning: Double-check that the text extracted via i.text works correctly with the existing re.sub, float() conversions, and market cap parsing logic. Small differences in whitespace or formatting between BS4's stripped_strings and requests-html's text might require minor adjustments to the regex patterns or cleaning steps.



Phase 4: Testing and Refinement

Run the Script: Execute the modified script.
Check for Errors: Look for errors during execution, especially in the render() step, element finding (find), and data conversion (float()).
Compare Output: Run the original BS4 version and the new requests-html version (if possible, against the same data fetched close in time). Compare the resulting tg_df0 DataFrames. Check for:
Correct number of rows.
Correct data in each column.
Correct data types.
Identical handling of edge cases (N/A, +/- signs, M/B/T suffixes).

Performance: Note the execution time. r.html.render() adds overhead as it runs a headless browser instance.
Iterate: Debug and refine the selectors, text extraction, and cleaning logic based on testing results until the output matches the original or is confirmed correct. Add more robust error handling (e.g., try-except blocks around find, text processing, and type conversions).



Phase 5: Code Cleanup
Remove Unused Code: Delete the BeautifulSoup import and any related variables (self.soup) that are no longer used.
Update Comments: Modify comments to accurately reflect the use of requests-html, CSS selectors, and .text extraction.
Review Logging: Ensure log messages are accurate and helpful.
